############### import ###############
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error
from sklearn import linear_model

import pulp 

import time,sys,copy,itertools,math

####################################################
# IMPORTANT:
# Please specify the path of the cplex solver here
# CPLEX_PATH= \
# "/Applications/CPLEX_Studio1210/cplex/bin/x86-64_osx/cplex"
CPLEX_PATH= \
"/opt/cplex_12.10/cplex/bin/x86-64_linux/cplex"
# CPLEX_PATH= \
# "/Applications/CPLEX_Studio128/cplex/bin/x86-64_osx/cplex"

CPLEX_MSG = False
CPLEX_TIMELIMIT = 0
solver_type = 1 # change 1 to 2 if use CBC solver
std_eps = 1e-5
####################################################

############### global variables ###############
CV = 5 # number of folds in cross-validation
REG_SEED = [3]
SPLIT_SEED = [2,3,4,5,6,7,8,9,10,11]   # 4 is used in the previous version
# SPLIT_SEED = [1]   # 4 is used in the previous version
R_key = ["R2train", "R2test", "R2all", "MAEtrain", "MAEtest", "MAEall", "time", "reg", "K"]
Usage = '''
usage: {} (train_ftr)(train_value)(output)(maxitr)(early)
                  
  - train_ftr   ... CSV file generated by our feature vector generator in Module 1.

  - train_value ... CSV file that contains target values.
    = The first column must contain CIDs.
    = The second column must contain target values and be named \'a\'. 
    = All CIDs appearing in train_ftr must be contained in train_value. 

  - output_{{biases,weights}}.txt will be output.
    They contain data on the constructed ANN. 

  - maxitr ... # of iterations in neural network learning

  - early ... Whether you use (1) early_stop in MLPRegressor or not (0)

'''.format(sys.argv[0])


def prepare_variables_Lasso_var1(
    K, train
):
    w = {(q, i): pulp.LpVariable(f"w({q},{i})", 0, cat=pulp.LpContinuous) for q in range(1) for i in range(1, K + 1)}
    b = pulp.LpVariable(f"b", cat=pulp.LpContinuous)
    # alpha = {i: pulp.LpVariable(f"alpha({i})", cat=pulp.LpContinuous) for i in range(1, 3)}

    b_bar = pulp.LpVariable(f"b_bar", 0, cat=pulp.LpContinuous)
    Delta = {j: pulp.LpVariable(f"Delta({j})", 0, cat=pulp.LpContinuous) for j in range(len(train))}

    return w, b, b_bar, Delta

def build_constraints_Lasso_var1(
    MILP,
    x_train, y_train, K,
    llambda, epsilon, I_minus, I_plus, selected,
    w, b, alpha, b_bar, Delta
):
    # for j in range(1, K + 1):
    #     if epsilon[(0, j)] == 0:
    #         MILP += w[(0, j)] == 0, f"Lasso_var1_1_{j}_{0}"
    #     for q in range(1, 3):
    #         if epsilon[(0, j)] != 0:
    #             MILP += w[(q, j)] == epsilon[(q, j)] / epsilon[(0, j)] * w[(0, j)], f"Lasso_var1_1_{j}_{q}"
    #         else:
    #             MILP += w[(q, j)] == 0, f"Lasso_var1_1_{j}_{q}"
    for i in range(len(x_train)):
        MILP += Delta[i] >= alpha[0] * y_train[i] + alpha[1] * (y_train[i] ** 2) + alpha[2] * (1 - (y_train[i] - 1) ** 2) - \
                pulp.lpSum([w[(0, j)] * epsilon[(0, j)] * x_train[i][j - 1] + w[(0, j)] * epsilon[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(0, j)] * epsilon[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_plus if selected[j]]) + \
                pulp.lpSum([w[(0, j)] * epsilon[(0, j)] * x_train[i][j - 1] + w[(0, j)] * epsilon[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(0, j)] * epsilon[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_minus if selected[j]]) - b, f"Lasso_var_2_{i}_1"
        MILP += -Delta[i] <= alpha[0] * y_train[i] + alpha[1] * (y_train[i] ** 2) + alpha[2] * (1 - (y_train[i] - 1) ** 2) - \
                pulp.lpSum([w[(0, j)] * epsilon[(0, j)] * x_train[i][j - 1] + w[(0, j)] * epsilon[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(0, j)] * epsilon[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_plus if selected[j]]) + \
                pulp.lpSum([w[(0, j)] * epsilon[(0, j)] * x_train[i][j - 1] + w[(0, j)] * epsilon[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(0, j)] * epsilon[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_minus if selected[j]]) - b, f"Lasso_var_2_{i}_2"


    MILP += b_bar >= b, f"Lasso_var2_3_1"
    MILP += b_bar >= -b, f"Lasso_var2_3_2"

    # Target function
    MILP += 1 / (2 * len(x_train)) * pulp.lpSum([Delta[i] for i in range(len(x_train))]) + \
            llambda * (pulp.lpSum([w[(0, j)] * epsilon[(q, j)] for q in range(3) for j in range(1, K + 1)]) + b_bar)

    return MILP

def prepare_variables_Lasso_var1_pre(
    K, train
):
    w = {(q, i): pulp.LpVariable(f"w({q},{i})", 0, cat=pulp.LpContinuous) for q in range(3) for i in range(1, K + 1)}
    b = pulp.LpVariable(f"b", cat=pulp.LpContinuous)
    alpha = {i: pulp.LpVariable(f"alpha({i})", 0, cat=pulp.LpContinuous) for i in range(3)}
    # alpha = {i: pulp.LpVariable(f"alpha({i})", 0, cat=pulp.LpContinuous) for i in range(1, 3)}

    b_bar = pulp.LpVariable(f"b_bar", 0, cat=pulp.LpContinuous)
    Delta = {j: pulp.LpVariable(f"Delta({j})", 0, cat=pulp.LpContinuous) for j in range(len(train))}

    return w, b, alpha, b_bar, Delta

def build_constraints_Lasso_var1_pre(
    MILP,
    x_train, y_train, K,
    llambda, I_minus, I_plus,
    w, b, alpha, b_bar, Delta
):

    MILP += alpha[0] + alpha[1] + alpha[2] == 1, "Lassp_var1"

    # for j in range(1, K + 1):
    #     for q in range(1, 3):
    #         MILP += w[(q, j)] <= epsilon[(q, j)] * w[(0, j)], f"Lasso_var1_1_{j}_{q}"
    for i in range(len(x_train)):
        MILP += Delta[i] >= alpha[0] * y_train[i] + alpha[1] * (y_train[i] ** 2) + alpha[2] * (1 - (y_train[i] - 1) ** 2) - \
                pulp.lpSum([w[(0, j)] * x_train[i][j - 1] + w[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_plus]) + \
                pulp.lpSum([w[(0, j)] * x_train[i][j - 1] + w[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_minus]) - b, f"Lasso_var_2_{i}_1"
        MILP += -Delta[i] <= alpha[0] * y_train[i] + alpha[1] * (y_train[i] ** 2) + alpha[2] * (1 - (y_train[i] - 1) ** 2) - \
                pulp.lpSum([w[(0, j)] * x_train[i][j - 1] + w[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_plus]) + \
                pulp.lpSum([w[(0, j)] * x_train[i][j - 1] + w[(1, j)] * (x_train[i][j - 1] ** 2) +
                        w[(2, j)] * (1 - (x_train[i][j - 1] - 1) ** 2)
                    for j in I_minus]) - b, f"Lasso_var_2_{i}_2"

    MILP += b_bar >= b, f"Lasso_var2_3_1"
    MILP += b_bar >= -b, f"Lasso_var2_3_2"

    # Target function
    MILP += 1 / (2 * len(x_train)) * pulp.lpSum([Delta[i] for i in range(len(x_train))]) + \
            llambda * (pulp.lpSum([w[(q, j)] for q in range(3) for j in range(1, K + 1)]) + b_bar)

    return MILP

############################################################
def linear_reg_Lasso_pre(descriptors_filename, target_values_filename,
              reg_seed, split_seed, T, early, llambda):

    ########## preprocess ##########
    ### read files ###
    # read the training and target data
    fv = pd.read_csv(descriptors_filename) 
    value = pd.read_csv(target_values_filename)      

    
    ### prepare training set ###
    # prepare CIDs
    CIDs = np.array(fv['CID'])
    # prepare target, train, test arrays
    target = np.array(value['a'])
    # construct dictionary: CID to feature vector
    fv_dict = {}
    for cid,row in zip(CIDs, fv.values[:,1:]):
        fv_dict[cid] = row
    # construct dictionary: CID to target value
    target_dict = {}
    for cid, val in zip(np.array(value['CID']), np.array(value['a'])):
        target_dict[cid] = val
    # check CIDs: target_values_filename should contain all CIDs that appear in descriptors_filename
    for cid in CIDs:
        if cid not in target_dict:
            sys.stderr.write('error: {} misses the target value of CID {}\n'.format(target_values_filename, cid))
            exit(1)
    # construct x and y so that the CIDs are ordered in ascending order
    CIDs.sort()
    max_a = 1
    x = np.array([fv_dict[cid] for cid in CIDs])
    ############ max_a = 2
    y = np.array([max_a * target_dict[cid] for cid in CIDs])

    # llambda /= 2 * len(y)

    # y = np.array([_y + 0.5*(_y ** 2) for _y in y])
    # obtain numbers of examples and features
    numdata = x.shape[0]
    numfeature = x.shape[1]
    '''
    # output stats (legacy)
    print('# {} contains {} vectors for {} (=CID+{}) features.'.format(descriptors_filename, fv.shape[0], fv.shape[1], fv.shape[1]-1)) 
    print('# {} contains {} target values.'.format(target_values_filename, value.shape[0]))
    print('# n range = [{},{}]'.format(fv['n'].min(),fv['n'].max()))
    print('# a range = [{},{}]'.format(min(y), max(y)))
    
    print('# instances = {}'.format(numdata))
    print('# features = {}'.format(numfeature))
    '''

    ### prepare learning ###

    reg_Lasso = linear_model.LinearRegression()
    # reg_Lasso.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
    # print(reg_Lasso.coef_)

    # initalize array that stores the result
    R_Lasso = {} # R[<key>][fold][t]
    for key in R_key:
        R_Lasso[key] = []
        for fold in range(CV):
            R_Lasso[key].append(dict())
    # separate the data randomly for cross-validation
    # kf = KFold(n_splits=CV, shuffle=True, random_state=split_seed)

    I_minus = list()
    I_plus = list()
    for j in range(1, numfeature + 1):
        x_j = [x[i][j - 1] for i in range(0, numdata)]
        corr = np.corrcoef(x_j, y)
        if corr[0][1] > 0:
            I_plus.append(j)
        if corr[0][1] < 0:
            I_minus.append(j)

    #########################################################
    MILP = pulp.LpProblem("MILP_ELR_var1", pulp.LpMinimize)
    # epsilon = {(q, j): epsilon_ub for q in range(1, 3) for j in range(numfeature + 1)}
    w, b, alpha, b_bar, Delta = prepare_variables_Lasso_var1_pre(numfeature, x)
    MILP = build_constraints_Lasso_var1_pre(
        MILP, 
        x, y, numfeature, 
        llambda, I_minus, I_plus, 
        w, b, alpha, b_bar, Delta
    )

    MILP.writeLP("test.lp")

    num_vars = len(MILP.variables())
    num_ints = len([var for var in MILP.variables() if var.cat == pulp.LpInteger])
    bins = [v.name for v in MILP.variables()
                                if (v.cat == pulp.LpBinary or
                                (v.cat == "Integer" and
                                v.upBound and v.lowBound != None and
                                round(v.upBound - v.lowBound) == 1))]
    
    reals = [v.name for v in MILP.variables() if v.cat == "Continuous"]
    
    num_constraints = len(
                [c for c in MILP.constraints.items()])
    
    # print("Number of variables:", num_vars)
    # print(" - Integer :", num_ints)
    # print(" - Binary  :", len(bins))
    # print("Number of constraints:", num_constraints)
    
    # Solve MILP
    if solver_type == 1:
        if CPLEX_TIMELIMIT > 0:
            CPLEX = pulp.CPLEX(path = CPLEX_PATH,
                               msg = CPLEX_MSG,
                               timeLimit = CPLEX_TIMELIMIT)
        else:
            CPLEX = pulp.CPLEX(path = CPLEX_PATH,
                               msg = CPLEX_MSG)
        # print("Start Solving Using CPLEX...")
        MILP.solve(CPLEX)
        solve_end = time.time()
    else:
        # print("Start Solving Using Coin-OR...")
        MILP.solve()
        solve_end = time.time()

    # for j in range(1, numfeature + 1):
    #     for q in range(3):
    #         print(f"w({q}, {j}) = {w[(q, j)].value()}")

    epsilon_result = {(q, j): w[(q, j)].value() / (w[(0, j)].value() + w[(1, j)].value() + w[(2, j)].value()) 
                            if abs(w[(0, j)].value() + w[(1, j)].value() + w[(2, j)].value()) > 1e-8 else 0 
                        for q in range(0, 3) for j in range(1, numfeature + 1)}
    selected = {j: True if abs(w[(0, j)].value() + w[(1, j)].value() + w[(2, j)].value()) >= 1e-8 else False for j in range(1, numfeature + 1)}
    alpha_result = {i: alpha[i].value() for i in range(0, 3)}


    print(f"alpha(0): {alpha[0].value()}, alpha(1): {alpha[1].value()}, alpha(2): {alpha[2].value()}")
    for j in range(1, numfeature + 1):
        if w[(0, j)].value() + w[(1, j)].value() + w[(2, j)].value() == 0:
            continue
        print(f"w(0, {j}) = {w[(0, j)].value() + w[(1, j)].value() + w[(2, j)].value()}, epsilon({j}, 0) = {epsilon_result[(0, j)]}, epsilon({j}, 1) = {epsilon_result[(1, j)]}, epsilon({j}, 2) = {epsilon_result[(2, j)]}")

    for j in range(numdata):
        print(f"Delta({j}) = {Delta[j].value()}")
    print(MILP.objective.value())

    c1 = pulp.lpSum(w[(q, j)].value() for q in range(1, 3) for j in range(1, numfeature + 1))
    c2 = pulp.lpSum(w[(0, j)].value() for j in range(1, numfeature + 1))
    print(f"c1 = {c1}, c2 = {c2}")
    try:
        print("Ratio: ", c1 / c2)
    except ZeroDivisionError as e:
        print('catch ZeroDivisionError:', e)
    
    #########################################################

    reg_Lasso.warm_start = False
    reg_Lasso.fit(x, y)

    y_tmp = np.array([alpha_result[0] * _y + alpha_result[1] * (_y ** 2) + alpha_result[2] * (1 - (_y - 1) ** 2) for _y in y])
    # print(y_tmp)

    x_tmp = np.array([[epsilon_result[(0, j)] * _x[j - 1] + epsilon_result[(1, j)] * (_x[j - 1] ** 2) + epsilon_result[(2, j)] * (1 - (_x[j - 1] - 1) ** 2)
                for j in range(1, numfeature + 1)] for _x in x])
    # x_tmp = np.array(
    #     [[_x[j - 1] + epsilon_result[(1, j)] * (_x[j - 1] ** 2) + epsilon_result[(2, j)] * (_x[j - 1]  ** 3)
    #       for j in range(1, numfeature + 1)] for _x in x])

    reg_Lasso.intercept_ = b.value()
    reg_Lasso.coef_ = np.array([(w[(0, j)].value() + w[(1, j)].value() + w[(2, j)].value()) if j in I_plus else -(w[(0, j)].value() + w[(1, j)].value() + w[(2, j)].value()) if j in I_minus else 0 for j in range(1, numfeature + 1)])

    # print(reg_Lasso.coef_)

    pred_tmp = reg_Lasso.predict(x_tmp)
    R2_score = reg_Lasso.score(x_tmp, y_tmp)

    print(f"Pre exp: R2 = {R2_score}")

    return R_Lasso, R2_score, epsilon_result, alpha_result, selected, I_minus, I_plus, numfeature


def linear_reg_Lasso(descriptors_filename, target_values_filename,
              reg_seed, split_seed, T, early, llambda, epsilon_result, alpha_result, selected, I_minus, I_plus):

    ########## preprocess ##########
    ### read files ###
    # read the training and target data
    fv = pd.read_csv(descriptors_filename) 
    value = pd.read_csv(target_values_filename)      

    
    ### prepare training set ###
    # prepare CIDs
    CIDs = np.array(fv['CID'])
    # prepare target, train, test arrays
    target = np.array(value['a'])
    # construct dictionary: CID to feature vector
    fv_dict = {}
    for cid,row in zip(CIDs, fv.values[:,1:]):
        fv_dict[cid] = row
    # construct dictionary: CID to target value
    target_dict = {}
    for cid, val in zip(np.array(value['CID']), np.array(value['a'])):
        target_dict[cid] = val
    # check CIDs: target_values_filename should contain all CIDs that appear in descriptors_filename
    for cid in CIDs:
        if cid not in target_dict:
            sys.stderr.write('error: {} misses the target value of CID {}\n'.format(target_values_filename, cid))
            exit(1)
    # construct x and y so that the CIDs are ordered in ascending order
    CIDs.sort()
    max_a = 1
    x = np.array([fv_dict[cid] for cid in CIDs])
    ############ max_a = 2
    y = np.array([max_a * target_dict[cid] for cid in CIDs])
    # y = np.array([_y + _y ** 2 for _y in y])

    # llambda /= 2 * len(y)

    # obtain numbers of examples and features
    numdata = x.shape[0]
    numfeature = x.shape[1]
    '''
    # output stats (legacy)
    print('# {} contains {} vectors for {} (=CID+{}) features.'.format(descriptors_filename, fv.shape[0], fv.shape[1], fv.shape[1]-1)) 
    print('# {} contains {} target values.'.format(target_values_filename, value.shape[0]))
    print('# n range = [{},{}]'.format(fv['n'].min(),fv['n'].max()))
    print('# a range = [{},{}]'.format(min(y), max(y)))
    
    print('# instances = {}'.format(numdata))
    print('# features = {}'.format(numfeature))
    '''

    ### prepare learning ###

    reg_Lasso = linear_model.LinearRegression()
    # reg_Lasso.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
    # print(reg_Lasso.coef_)

    # initalize array that stores the result
    R_Lasso = {} # R[<key>][fold][t]
    for key in R_key:
        R_Lasso[key] = []
        for fold in range(CV):
            R_Lasso[key].append(dict())
    # separate the data randomly for cross-validation
    kf = KFold(n_splits=CV, shuffle=True, random_state=split_seed)

    fold = -1
    ### start learning experiments ###
    for train, test in kf.split(x):
        fold += 1
        x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]
        reg_Lasso.warm_start = False
        start = time.time()
        print("\n\n# reg_seed={}, split_seed={}, fold={}".format(reg_seed, split_seed, fold))
        # learn ANN, but stop the learning at itr=t in order to record stats
        # for t in T:
        t = 0
        #     reg_Lasso.max_iter = t
        reg_Lasso.fit(x_train, y_train)

        #########################################################
        MILP = pulp.LpProblem("MILP_Lasso_var1", pulp.LpMinimize)
        w, b, b_bar, Delta = prepare_variables_Lasso_var1(numfeature, x_train)
        MILP = build_constraints_Lasso_var1(
            MILP,
            x_train, y_train, numfeature,
            llambda, epsilon_result, I_minus, I_plus, selected,
            w, b, alpha_result, b_bar, Delta
        )

        num_vars = len(MILP.variables())
        num_ints = len([var for var in MILP.variables() if var.cat == pulp.LpInteger])
        bins = [v.name for v in MILP.variables()
                                    if (v.cat == pulp.LpBinary or
                                    (v.cat == "Integer" and
                                    v.upBound and v.lowBound != None and
                                    round(v.upBound - v.lowBound) == 1))]

        reals = [v.name for v in MILP.variables() if v.cat == "Continuous"]

        num_constraints = len(
                    [c for c in MILP.constraints.items()])

        # print("Number of variables:", num_vars)
        # print(" - Integer :", num_ints)
        # print(" - Binary  :", len(bins))
        # print("Number of constraints:", num_constraints)

        # Solve MILP
        if solver_type == 1:
            if CPLEX_TIMELIMIT > 0:
                CPLEX = pulp.CPLEX(path = CPLEX_PATH,
                                   msg = CPLEX_MSG,
                                   timeLimit = CPLEX_TIMELIMIT)
            else:
                CPLEX = pulp.CPLEX(path = CPLEX_PATH,
                                   msg = CPLEX_MSG)
            # print("Start Solving Using CPLEX...")
            MILP.solve(CPLEX)
            solve_end = time.time()
        else:
            # print("Start Solving Using Coin-OR...")
            MILP.solve()
            solve_end = time.time()

        #########################################################

        reg_Lasso.intercept_ = b.value()
        reg_Lasso.coef_ = np.array([w[(0, j)].value() if j in I_plus and selected[j] else -w[(0, j)].value() if j in I_minus and selected[j] else 0 for j in range(1, numfeature + 1)])

        y_train = np.array([alpha_result[0] * _y + alpha_result[1] * (_y ** 2) + alpha_result[2] * (1 - (_y - 1) ** 2) for _y in y_train])
        y_test = np.array([alpha_result[0] * _y + alpha_result[1] * (_y ** 2) + alpha_result[2] * (1 - (_y - 1) ** 2) for _y in y_test])

        # y_train = np.array([_y + alpha_result[1] * (_y ** 2) + alpha_result[2] * (_y ** 3) for _y in y_train])
        # y_test = np.array([_y + alpha_result[1] * (_y ** 2) + alpha_result[2] * (_y ** 3) for _y in y_test])

        x_train = np.array([[epsilon_result[(0, j)] * _x[j - 1] + epsilon_result[(1, j)] * (_x[j - 1] ** 2) + epsilon_result[(2, j)] * (1 - (_x[j - 1] - 1) ** 2)
                    for j in range(1, numfeature + 1)] for _x in x_train])
        x_test = np.array([[epsilon_result[(0, j)] * _x[j - 1] + epsilon_result[(1, j)] * (_x[j - 1] ** 2) + epsilon_result[(2, j)] * (1 - (_x[j - 1] - 1) ** 2)
                    for j in range(1, numfeature + 1)] for _x in x_test])

        # x_train = np.array(
        #     [[_x[j - 1] + epsilon_result[(1, j)] * (_x[j - 1] ** 2) + epsilon_result[(2, j)] * (_x[j - 1] ** 3)
        #       for j in range(1, numfeature + 1)] for _x in x_train])
        # x_test = np.array(
        #     [[_x[j - 1] + epsilon_result[(1, j)] * (_x[j - 1] ** 2) + epsilon_result[(2, j)] * (_x[j - 1] ** 3)
        #       for j in range(1, numfeature + 1)] for _x in x_test])

            # reg_Lasso.warm_start = True
            # obtain the prediction to compute MAE
        pred = reg_Lasso.predict(x)
        pred_train = reg_Lasso.predict(x_train)
        pred_test = reg_Lasso.predict(x_test)
            # calculate the prediction score (R^2)
        R_Lasso["R2train"][fold][t] = reg_Lasso.score(x_train,y_train)
        R_Lasso["R2test"][fold][t] = reg_Lasso.score(x_test,y_test)
        R_Lasso["R2all"][fold][t] = reg_Lasso.score(x,y)
            # calculate MAE
        R_Lasso["MAEtrain"][fold][t] = mean_absolute_error(y_train,pred_train)
        R_Lasso["MAEtest"][fold][t] = mean_absolute_error(y_test,pred_test)
        R_Lasso["MAEall"][fold][t] = mean_absolute_error(y,pred)
            # store time and ref
        R_Lasso["time"][fold][t] = time.time() - start
        R_Lasso["reg"][fold][t] = copy.deepcopy(reg_Lasso)

        K = 0
        for tmp in reg_Lasso.coef_:
            if abs(tmp) > 1e-10:
                K += 1

        R_Lasso["K"][fold][t] = K

        print("{}\t{}\t{}\t{}\t{}".format(t, R_Lasso["R2train"][fold][t], R_Lasso["R2test"][fold][t], R_Lasso["time"][fold][t], K))

    return R_Lasso

def get_best_cv_and_t(Result, key, T):
    best_r = best_fold = best_t = best_ts = best_trial = None
    avg_all = None
    avg_all_tmp = list()
    mid_all = None
    K_all = list()
    tim_all = list()
    for r in Result.keys():
        R = Result[r]
        for t in T:
            x = [R[key][fold][t] for fold in range(CV)]
            for _x in x:
                avg_all_tmp.append(_x)
            K = [R["K"][fold][t] for fold in range(CV)]
            tim = [R["time"][fold][t] for fold in range(CV)]
            for _K in K:
                K_all.append(_K)
            for _tim in tim:
                tim_all.append(_tim)
            avg = np.mean(x)
            if best_ts is None or avg > best_ts:
                best_r = r
                best_t = t
                best_ts = avg
                best_trial = max(x)
                best_fold = x.index(best_trial)
    avg_all = np.mean(avg_all_tmp)
    mid_all = np.median(avg_all_tmp)
    min_all = np.min(avg_all_tmp)
    max_all = np.max(avg_all_tmp)
    avg_K = np.mean(K_all)
    avg_tim = np.mean(tim_all)
            
    return (best_r, best_fold, best_t, best_ts, best_trial, avg_all, mid_all, min_all, max_all, avg_K, avg_tim)

############################################################
def main(argv):
    # if len(argv)<6:
    #     print(Usage)
    #     sys.exit()

    ##### Parse the command line arguments #####
    descriptors_filename = argv[1]
    target_values_filename = argv[2]
    output_filename = argv[3]
    maxitr = int(argv[4])
    early = False if int(argv[5])==0 else True
    llambda = float(argv[6])
    epsilon_ub = float(argv[7])
    # architecture = tuple(int(a) for a in argv[6:])
    
    # initialize recording step
    # T = [t for t in range(100, min(1000, maxitr), 100)]
    # T += [t for t in range(1000, maxitr+1, 1000)]
    T = [0]
    
    ##### Perform 5-fold validation with the given training data #####
    Result = {}
    SEED = list(itertools.product(REG_SEED, SPLIT_SEED))

    _, R2_score, epsilon_result, alpha_result, selected, I_minus, I_plus, numfeature = linear_reg_Lasso_pre(descriptors_filename,
                                                   target_values_filename,
                                                   3, 1,
                                                   T, early, llambda
                                                   )

    for (reg_seed, split_seed) in SEED:
        Result[(reg_seed, split_seed)] = linear_reg_Lasso(descriptors_filename,
                                                   target_values_filename,
                                                   reg_seed, split_seed,
                                                   T, early, llambda, epsilon_result, alpha_result, selected,
                                                   I_minus, I_plus
                                                   )


    (best_r, best_fold, best_t, best_ts, best_trial, avg_all, mid_all, min_all, max_all, avg_K, avg_tim) = get_best_cv_and_t(Result, "R2test", T)
    
    ANN_seed, split_seed = best_r
    tr = Result[(ANN_seed,split_seed)]["R2train"][best_fold][best_t]
    tim = Result[(ANN_seed,split_seed)]["time"][best_fold][best_t]
    print("\n\n# ========== COMPLETE ==========")
    print("# BEST_R2_CV:\t{}".format(best_ts))
    print("# BEST_R2_TEST_PER_TRIAL:\t{}".format(best_trial))
    print("# R2_TRAIN_FOR_THAT:\t{}".format(tr))
    print("# TIME:\t{}".format(tim))
    print("# REG_SEED:\t{}".format(ANN_seed))
    print("# SPLIT_SEED:\t{}".format(split_seed))
    print("# FOLD:\t{}".format(best_fold))
    print("# ITR:\t{}".format(best_t))
    print("Avg for all:\t{}".format(avg_all))
    print("Med for all:\t{}".format(mid_all))

    # print(f"lambda = {llamdba}, epsilon_ub = {epsilon_ub}, pre-R2_test = {R2_score_test}")
    # print(R2_score_test)

    # print(R2_score)
    # print(avg_all)
    # print(mid_all)
    # print(min_all)
    # print(max_all)
    # print(avg_K)
    # print(avg_tim)

    best_regressor = Result[(ANN_seed,split_seed)]["reg"][best_fold][best_t]

    with open(output_filename + f"{llambda}_var1_elinreg.txt", 'w', newline='\n') as f:
        f.write(f"# Avg for all:\t{avg_all}\n")
        f.write(f"# Med for all:\t{mid_all}\n")
        f.write(f"{numfeature}\n")
        for x in best_regressor.coef_:
            f.write(f"{x} ")
        f.write("\n")
        for ss in range(3):
            for i in range(1, numfeature + 1):
                f.write(f"{epsilon_result[(ss, i)]} ")
            f.write("\n")
        f.write(f"{best_regressor.intercept_}\n")
        f.write(f"{alpha_result[0]} {alpha_result[1]} {alpha_result[2]}\n")
        f.close()

if __name__=="__main__":
    # l = 0.001
    # epsilon = 100
    # prop = "At_large"
    main(sys.argv)
    # main((0, "homo_desc_norm.csv", "homo_values.txt", "test", "10000", "0"))
    # main((0, f"./data_mass_avg/{prop}/{prop}_desc_norm.csv", f"./data_mass_avg/{prop}/{prop}_norm_values.txt", "test", "10000", "0", f"{l}", f"{epsilon}"))
    